{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient Descent (Desent/Turun Gradien) adalah algoritma optimasi yang digunakan untuk mencari nilai minimum (atau maksimum) dari sebuah fungsi. Algoritma ini sering digunakan dalam machine learning dan deep learning untuk mengoptimalkan model dengan cara meminimalkan fungsi kerugian (loss function) atau fungsi objektif.\n",
    "\n",
    "Berikut adalah konsep dasar dari Gradient Descent:\n",
    "\n",
    "Fungsi Objektif (Loss Function): Gradient Descent digunakan untuk mengoptimalkan fungsi objektif yang biasanya mengukur seberapa baik model bekerja dalam memprediksi atau memodelkan data. Tujuan utama adalah untuk menemukan parameter-parameter yang meminimalkan fungsi objektif ini.\n",
    "\n",
    "Gradien (Gradient): Gradien adalah vektor yang menunjukkan arah dan tingkat pertumbuhan paling cepat dari fungsi objektif pada suatu titik tertentu. Gradien mengindikasikan di mana perubahan nilai parameter akan memberikan penurunan terbesar dalam fungsi objektif. Untuk mencari nilai minimum fungsi, kita mencari titik di mana gradiennya nol.\n",
    "\n",
    "Iteratif: Gradient Descent adalah algoritma iteratif. Itu berarti kita memulai dengan perkiraan awal untuk parameter-parameter kita dan terus memperbarui parameter tersebut dalam setiap iterasi sampai kita mencapai kondisi berhenti yang ditentukan (misalnya, jumlah iterasi maksimum atau perubahan fungsi objektif yang cukup kecil).\n",
    "\n",
    "Langkah (Learning Rate): Learning rate (tingkat pembelajaran) adalah hyperparameter yang mengatur seberapa besar langkah yang akan diambil pada setiap iterasi. Ini memengaruhi seberapa cepat atau lambat algoritma konvergen ke nilai minimum. Jika learning rate terlalu besar, algoritma bisa melewati minimumnya, sedangkan jika terlalu kecil, algoritma bisa berjalan sangat lambat atau terjebak dalam minimum lokal.\n",
    "\n",
    "Langkah-langkah umum dalam Gradient Descent adalah sebagai berikut:\n",
    "\n",
    "*Hitung gradien fungsi objektif terhadap parameter saat ini.\n",
    "*Update parameter-parameter tersebut dengan mengambil langkah berlawanan dengan gradien (untuk meminimalkan fungsi objektif).\n",
    "*Ulangi langkah-langkah ini sampai kondisi berhenti terpenuhi.\n",
    "\n",
    "Ada beberapa varian dari Gradient Descent, seperti Stochastic Gradient Descent (SGD) yang hanya menggunakan satu sampel pelatihan pada setiap iterasi, Mini-Batch Gradient Descent yang menggunakan sejumlah kecil sampel, dan Batch Gradient Descent yang menggunakan semua data pelatihan dalam setiap iterasi. Selain itu, ada algoritma optimasi yang lebih canggih yang memodifikasi langkah-langkah ini, seperti Adam dan RMSProp, untuk membantu mengatasi beberapa masalah yang bisa terjadi dalam algoritma dasar Gradient Descent.\n",
    "\n",
    "Tujuan utama dari Gradient Descent adalah untuk menemukan parameter yang meminimalkan (atau memaksimalkan) fungsi objektif, sehingga model Anda dapat melakukan prediksi atau pembelajaran yang lebih baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-linux_x86_64.whl\n",
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inisiasi variabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prediksi model\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "# loss function (dengan MSE)\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menghitung Gradient\n",
    "Gradient dihitung dengan turunan dari loss function $J(\\theta)$ (kita menggunakan contoh Mean Squared Error), dengan $\\theta$ adalah parameter model (sering disebut juga sebagai bobot atau dalam simbol $w$)\n",
    "$$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n}(\\theta*x-y)^2$$\n",
    "Sehingga:\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\frac{1}{n} \\sum_{i=1}^{n} 2x(\\theta*x-y)    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediksi sebelum training: f(5) = 0.000\n"
     ]
    }
   ],
   "source": [
    "def gradient(x, y, y_pred):\n",
    "    return np.dot(2 * x, y_pred - y).mean()\n",
    "\n",
    "\n",
    "# Mengecek nilai prediksi awal\n",
    "print(f'Prediksi sebelum training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melakukan Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: w = 1.200, loss = 30.000\n",
      "Epoch 2: w = 1.680, loss = 4.800\n",
      "Epoch 3: w = 1.872, loss = 0.768\n",
      "Epoch 4: w = 1.949, loss = 0.123\n",
      "Epoch 5: w = 1.980, loss = 0.020\n",
      "Epoch 6: w = 1.992, loss = 0.003\n",
      "Epoch 7: w = 1.997, loss = 0.001\n",
      "Epoch 8: w = 1.999, loss = 0.000\n",
      "Epoch 9: w = 1.999, loss = 0.000\n",
      "Epoch 10: w = 2.000, loss = 0.000\n",
      "Prediksi setelah training: f(5) = 9.999\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # forward pass\n",
    "    y_pred = forward(X)\n",
    "    # backward pass\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # menghitung gradient\n",
    "    grad = gradient(X, Y, y_pred)\n",
    "\n",
    "    # update nilai weight\n",
    "    w -= learning_rate * grad\n",
    "    print(f'Epoch {epoch + 1}: w = {w:.3f}, loss = {l:.3f}')\n",
    "\n",
    "# prediksi setelah training\n",
    "print(f'Prediksi setelah training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dapat diamati bahwa setiap proses training, loss akan semakin kecil\n",
    "- Setiap perulangan, algoritma akan memperbaharui nilai weight hingga nilai loss menjadi 0\n",
    "- Selanjutnya mari implementasi cara diatas dengan PyTorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
